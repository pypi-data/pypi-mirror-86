# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00_algorithms.ipynb (unless otherwise specified).

__all__ = ['eaSimpleWithExtraLog', 'eaMuPlusLambdaWithExtraLog', 'eaMuCommaLambdaWithExtraLog', 'ScikitLearner',
           'ScikitLearnerCV']

# Cell
from deap import algorithms
from deap import base
from deap import creator
from deap import tools
from deap import algorithms
from functools import partial
def eaSimpleWithExtraLog(population, toolbox, cxpb, mutpb, ngen, stats=None,
             halloffame=None, elitism=False, verbose=__debug__):
    """This algorithm add extra log and elitism to deap eaSimple algorithm.

    **Parameters**

    - population:
        A list of individuals.
    - toolbox:
        A `~deap.base.Toolbox` that contains the evolution operators.
    - cxpb:
        The probability of mating two individuals.
    - mutpb:
        The probability of mutating an individual.
    - ngen:
        The number of generation.
    - stats:
         A `~deap.tools.Statistics` object that is updated inplace, optional.
    - halloffame:
        A `~deap.tools.HallOfFame` object that will contain the best individuals, optional.
    - elitism:
        Whether or not to keep the hall of fame in the offspring.
    - verbose:
        Whether or not to log the statistics.

    **Returns**

    - The final population
    - A `~deap.tools.Logbook` with the statistics of the evolution
    """
    logbook = tools.Logbook()
    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])

    # Evaluate the individuals with an invalid fitness
    invalid_ind = [ind for ind in population if not ind.fitness.valid]
    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        if len(fit)==2:
            # Monkey patch the attribute to ind.extra
            ind.fitness.values, ind.extra = fit
        else:
            ind.fitness.values = fit

    if halloffame is not None:
        halloffame.update(population)
    if elitism and halloffame is None:
        raise ValueError('elitism must used with halloffame.')
    if elitism:
        hof_size = len(halloffame.items)

    record = stats.compile(population) if stats else {}
    logbook.record(gen=0, nevals=len(invalid_ind), **record)
    if verbose:
        print(logbook.stream)

    # Begin the generational process
    for gen in range(1, ngen + 1):
        # Select the next generation individuals
        if elitism:
            offspring = toolbox.select(population, len(population) - hof_size)
        else:
            offspring = toolbox.select(population, len(population))

        # Vary the pool of individuals
        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)

        # Evaluate the individuals with an invalid fitness
        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)
        for ind, fit in zip(invalid_ind, fitnesses):
            if len(fit)==2:
                # Monkey patch the attribute to ind.extra
                ind.fitness.values, ind.extra = fit
            else:
                ind.fitness.values = fit

        # No need to recalculate fitness values for halloffame items.
        if elitism:
            offspring.extend(halloffame.items)

        # Update the hall of fame with the generated individuals
        if halloffame is not None:
            halloffame.update(offspring)

        # Replace the current population by the offspring
        population[:] = offspring

        # Append the current generation statistics to the logbook
        record = stats.compile(population) if stats else {}
        logbook.record(gen=gen, nevals=len(invalid_ind), **record)
        if verbose:
            print(logbook.stream)

    return population, logbook

# Cell
def eaMuPlusLambdaWithExtraLog(population, toolbox, mu, lambda_, cxpb, mutpb, ngen,
                   stats=None, halloffame=None, verbose=__debug__):
    """Simply modified version of `eaMuPlusLambda` to accept additional return from evaluate function,
    check `eaMuPlusLambda` for documentation."""
    logbook = tools.Logbook()
    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])

    # Evaluate the individuals with an invalid fitness
    invalid_ind = [ind for ind in population if not ind.fitness.valid]
    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        if len(fit)==2:
            # Monkey patch the attribute to ind.extra
            ind.fitness.values, ind.extra = fit
        else:
            ind.fitness.values = fit

    if halloffame is not None:
        halloffame.update(population)

    record = stats.compile(population) if stats is not None else {}
    logbook.record(gen=0, nevals=len(invalid_ind), **record)
    if verbose:
        print(logbook.stream)

    # Begin the generational process
    for gen in range(1, ngen + 1):
        # Vary the population
        offspring = algorithms.varOr(population, toolbox, lambda_, cxpb, mutpb)

        # Evaluate the individuals with an invalid fitness
        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)
        for ind, fit in zip(invalid_ind, fitnesses):
            if len(fit)==2:
                # Monkey patch the attribute to ind.extra
                ind.fitness.values, ind.extra = fit
            else:
                ind.fitness.values = fit

        # Update the hall of fame with the generated individuals
        if halloffame is not None:
            halloffame.update(offspring)

        # Select the next generation population
        population[:] = toolbox.select(population + offspring, mu)

        # Update the statistics with the new population
        record = stats.compile(population) if stats is not None else {}
        logbook.record(gen=gen, nevals=len(invalid_ind), **record)
        if verbose:
            print(logbook.stream)

    return population, logbook

# Cell
def eaMuCommaLambdaWithExtraLog(population, toolbox, mu, lambda_, cxpb, mutpb, ngen,
                    stats=None, halloffame=None, verbose=__debug__):
    """Simply modified version of `eaMuCommaLambda` to accept additional return from evaluate function,
    check `eaMuCommaLambda` for documentation.
    """
    assert lambda_ >= mu, "lambda must be greater or equal to mu."

    # Evaluate the individuals with an invalid fitness
    invalid_ind = [ind for ind in population if not ind.fitness.valid]
    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        if len(fit)==2:
            # Monkey patch the attribute to ind.extra
            ind.fitness.values, ind.extra = fit
        else:
            ind.fitness.values = fit

    if halloffame is not None:
        halloffame.update(population)

    logbook = tools.Logbook()
    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])

    record = stats.compile(population) if stats is not None else {}
    logbook.record(gen=0, nevals=len(invalid_ind), **record)
    if verbose:
        print(logbook.stream)

    # Begin the generational process
    for gen in range(1, ngen + 1):
        # Vary the population
        offspring = algorithms.varOr(population, toolbox, lambda_, cxpb, mutpb)

        # Evaluate the individuals with an invalid fitness
        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)
        for ind, fit in zip(invalid_ind, fitnesses):
            if len(fit)==2:
                # Monkey patch the attribute to ind.extra
                ind.fitness.values, ind.extra = fit
            else:
                ind.fitness.values = fit

        # Update the hall of fame with the generated individuals
        if halloffame is not None:
            halloffame.update(offspring)

        # Select the next generation population
        population[:] = toolbox.select(offspring, mu)

        # Update the statistics with the new population
        record = stats.compile(population) if stats is not None else {}
        logbook.record(gen=gen, nevals=len(invalid_ind), **record)
        if verbose:
            print(logbook.stream)
    return population, logbook


# Cell
import sklearn

from sklearn.metrics import *
from functools import lru_cache, partial


class ScikitLearner:
    """Helper class to use fine tune parameter based on validation dataset performace.
    Feel free to implement your own version."""
    def __init__(self, learner, predict_proba=True):
        """Create the class instance.

        **Parameters**

        - learner: the scikit-learn estimator
        - predict_proba: whether the estimator can predict probability
        """
        self.learner = learner
        self.predict_proba = predict_proba
        self._proba = None

    def __getattr__(self, key):
        return getattr(self.learner, key)

    def __dir__(self):
        return set(super().__dir__() + list(self.__dict__.keys()) +
                   dir(self.learner))

    @lru_cache(maxsize=None)
    def val_loss_with_params(self, loss_func=None, callbacks=tuple(), **params):
        """Calculate loss of the estimator on validation set, and also the values of callbacks.

        **Parameters**

        - loss_func: loss function to use. By default, if estimator support to predict probability, it will be
        log_loss, otherwise it will be accuracy score.
        - callbacks: callbacks to also evaluate. Default is empty tuple.
        - params: parameters to use when train the estimator.

        **Returns**

        Return values will be a two element tuple.
        - the first is values of the loss function
        - the other is values of all callbacks
        """
        return self.loss_with_params(self.X_val,
                                     self.y_val,
                                     loss_func=loss_func,
                                     callbacks=callbacks,
                                     **params)

    def loss_with_params(self, X, y, loss_func=None, callbacks=tuple(), **params):
        estimator = sklearn.clone(self.learner)
        estimator.set_params(**params)
        estimator.fit(self.X_train, self.y_train)
        if self.predict_proba:
            if self._proba is None:
                try:
                    y_pred = estimator.predict_proba(self.X_val)
                    self._proba = True
                except:
                    self._proba = False
                    y_pred = estimator.predict(self.X_val)
            elif self._proba:
                y_pred = estimator.predict_proba(self.X_val)
            else:
                y_pred = estimator.predict(self.X_val)
        else:
            y_pred = estimator.predict(self.X_val)
        if loss_func is None:
            if self._proba:
                loss_func = log_loss
            else:
                loss_func = lambda x,y: -accuracy_score(x,y)
        return [loss_func(self.y_val, y_pred)], [callback(self.y_val, y_pred) for callback in callbacks]


# Cell
import sklearn

from sklearn.metrics import *
from functools import lru_cache, partial
from sklearn.model_selection import cross_val_predict


class ScikitLearnerCV:
    "Make use of sklearn cross_val_predict interface to optimize paramters."

    def __init__(self, learner, X, y):
        """Create the class instance.

        **Parameters**

        - learner: the scikit-learn estimator
        - predict_proba: whether the estimator can predict probability
        """
        self.learner = learner
        self.X = X
        self.y = y

    def __getattr__(self, key):
        return getattr(self.learner, key)

    def __dir__(self):
        return set(super().__dir__() + list(self.__dict__.keys()) +
                   dir(self.learner))

    @lru_cache(maxsize=None)
    def cv_loss_with_params(self,
                             loss_func=None,
                             callbacks=tuple(),
                             groups=None,
                             cv=None,
                             n_jobs=None,
                             verbose=0,
                             pre_dispatch='2*n_jobs',
                             method='predict',
                             fit_params=None,
                             **params):
        """Calculate loss of the estimator on validation set, and also the values of callbacks.

        **Parameters**

        - loss_func: loss function to use. By default, if estimator support to predict probability, it will be
        log_loss, otherwise it will be accuracy score.
        - callbacks: callbacks to also evaluate. Default is empty tuple.
        - params: parameters to use when train the estimator.

        **Returns**

        Return values will be a two element tuple.
        - the first is values of the loss function
        - the other is values of all callbacks
        """
        return self.loss_with_params(self.X,
                                     self.y,
                                     loss_func=loss_func,
                                     callbacks=callbacks,
                                     **params)

    def loss_with_params(self,
                         X,
                         y,
                         loss_func=None,
                         callbacks=tuple(),
                         groups=None,
                         cv=None,
                         n_jobs=None,
                         verbose=0,
                         pre_dispatch='2*n_jobs',
                         method='predict',
                         fit_params=None,
                         **params):
        estimator = sklearn.clone(self.learner)
        estimator.set_params(**params)
        y_pred = cross_val_predict(estimator,
                                   X,
                                   y,
                                   groups=groups,
                                   cv=cv,
                                   n_jobs=n_jobs,
                                   verbose=verbose,
                                   pre_dispatch=pre_dispatch,
                                   fit_params=fit_params,
                                   method=method)
        if loss_func is None:
            if method == 'predict_proba':
                loss_func = log_loss
            else:
                loss_func = lambda x, y: -accuracy_score(x, y)
        return [loss_func(y, y_pred)
                ], [callback(y, y_pred) for callback in callbacks]
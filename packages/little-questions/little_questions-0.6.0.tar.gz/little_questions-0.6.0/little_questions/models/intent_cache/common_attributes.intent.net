FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 4.98667307044080188039e-01) (1, -1.96356519242476740272e-01) (2, -8.70119250618927447816e-02) (3, -1.28664343734931324592e-01) (4, -1.63155078073691645058e-01) (5, -6.41086475105172803879e-01) (6, 1.43233597561034153856e-01) (7, -1.07358127427564786771e+00) (8, 6.25931266631117866339e-02) (9, 1.80633400191809156965e+00) (10, -9.01008593238203925324e-02) (0, -3.10083142682753631902e+00) (1, -1.57516768344834495741e-01) (2, -4.74119701579270866679e-04) (3, -1.83632621684268165785e-01) (4, -7.20690434362442206018e-02) (5, 3.02213206698803471539e+00) (6, 6.55812947149484598564e-02) (7, 3.56794054625537526704e+00) (8, -4.92378432566912410673e-03) (9, -1.00527790841006261502e+01) (10, -2.04954073187679025603e-01) (0, 1.99631149743534380692e-01) (1, -1.84998312720970753409e-01) (2, -6.54533298108144756533e-02) (3, -1.65152562538103703238e-01) (4, -1.56780564913468006827e-01) (5, 4.35663223368213170517e-01) (6, 2.42248453167297567168e-01) (7, 5.98383732799318557305e-02) (8, 5.01065869728089252622e-02) (9, -8.12880988876656829234e-01) (10, -7.61000339602438263809e-02) (0, 1.07269490118434962511e+00) (1, 6.23457077538387083315e-01) (2, 7.54934105669871846622e-01) (3, 6.71950626766101399845e-01) (4, 6.27350389605180525088e-01) (5, 2.05563895152566233904e+00) (6, -7.73307189620601076463e-01) (7, 2.55436089801596422078e+00) (8, 7.43029037141159198754e-01) (9, 3.96485483077183875622e+00) (10, 9.65948014403978649778e-02) (0, -6.01103432450616273486e-02) (1, 5.62451708349300472101e-01) (2, 5.43405092540098277887e-01) (3, 6.06604061308217978876e-01) (4, 5.49699812414957134088e-01) (5, 1.97030285836588014980e+00) (6, -8.34612228251563448289e-01) (7, 2.30782374636431741877e+00) (8, 7.60372431599453113549e-01) (9, 4.05912524637595328869e+00) (10, 1.83313120145677382888e-01) (0, 2.02376021055552218764e-01) (1, -2.77068687780288036571e-01) (2, -4.01703482671168454754e-01) (3, -3.26835749848273515727e-01) (4, -4.38359586699393399822e-01) (5, 2.01077892777352307263e+00) (6, -1.61493968859729546128e-02) (7, 2.13479615511204912082e+00) (8, -1.64960091493968563636e-01) (9, 1.81732729588946329002e+00) (10, -6.43501502561950933234e-01) (0, 1.71886890876120496108e-01) (1, -6.39513338770572453695e-02) (2, -1.09569167610615941788e-01) (3, -1.10596490916222783829e-01) (4, -1.72930030044526228394e-01) (5, 9.25599211466997084941e-01) (6, 6.02819501653243494665e-01) (7, 1.31315656545326020499e+00) (8, -1.74478661951152691501e-01) (9, -6.36208667241646175228e+00) (10, -1.19305654730423135734e-01) (0, 4.50805127466729793628e-01) (1, -1.91694122988454812573e-01) (2, -7.18263064210338531002e-02) (3, -1.27952319819204352047e-01) (4, -1.61977869707815164135e-01) (5, -7.95607492907092650469e-01) (6, 2.62869999095114659227e-01) (7, -9.17050409738885630162e-01) (8, -3.39291480071830242604e-02) (9, 1.71843706125339878987e+00) (10, -4.50876630155803356792e-02) (0, 1.63289261965128745802e-01) (1, -3.64637082423334257797e-01) (2, -2.33767518754367992218e-01) (3, -2.85865554566745783127e-01) (4, -2.44452292080288097198e-01) (5, 1.57988871985452927760e+00) (6, 2.76123435791534810324e-01) (7, 2.50380117464682960815e+00) (8, 7.88700765692372463000e-01) (9, 7.24069108082697310280e-01) (10, -1.16835204369313983008e+00) (0, 9.66249818923828607264e-02) (1, 5.27378398184352947808e-01) (2, 4.80351614956432526604e-01) (3, 3.54639890794330780999e-01) (4, 4.30795232955986207024e-01) (5, -5.77697196435919435586e-01) (6, 2.04251857233040157169e+00) (7, -1.96824462739530803290e+00) (8, 1.75988820696101178065e+00) (9, -1.49682438518131144534e+00) (10, 5.40534496165148564684e-02) (11, -2.24935058965645640061e-01) (12, -5.33621015551948962852e-01) (13, -1.50624035866356337232e-01) (14, -2.40362626377336019434e-01) (15, -2.29842289624539869441e-01) (16, 8.69089904238771526757e-01) (17, -1.91419264945386025101e-01) (18, -1.95759758814535944627e-01) (19, 6.73841466736806449234e-01) (20, 5.41463372902446726798e-01) (21, 4.64477008274464697735e-01) 

Metadata-Version: 2.1
Name: transformer-as-service
Version: 1.13.4
Summary: Use your pretrained models as service
Home-page: UNKNOWN
Author: faith
Author-email: xianzixiang@gmail.com
License: MIT
Keywords: transformer pretrained model as service
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3.6
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Description-Content-Type: text/markdown
Requires-Dist: numpy
Requires-Dist: six
Requires-Dist: pyzmq (>=17.1.0)
Requires-Dist: GPUtil (>=1.3.0)
Requires-Dist: termcolor (>=1.1)
Requires-Dist: transformers (>=3.1.0)
Provides-Extra: cpu
Requires-Dist: tensorflow (>=1.10.0) ; extra == 'cpu'
Provides-Extra: gpu
Requires-Dist: tensorflow-gpu (>=1.10.0) ; extra == 'gpu'
Provides-Extra: http
Requires-Dist: fastapi ; extra == 'http'
Requires-Dist: flask-json ; extra == 'http'
Requires-Dist: transformer-as-client ; extra == 'http'

<h1 align="center">transformer-as-service</h1>

<p align="center">Using transformer pretrained model as a sentence encoding service, i.e. mapping a variable-length sentence to a fixed-length vector.</p>

Change: remove bert dependency.

Thanks for the idea from [bert-as-serivce](https://github.com/hanxiao/bert-as-service)

